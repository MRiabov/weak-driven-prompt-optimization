# Implementation Plan: 001-weak-driven-prompt-optimization-experiment
*Path: kitty-specs/001-weak-driven-prompt-optimization-experiment/plan.md*

**Branch**: `001-weak-driven-prompt-optimization-experiment` | **Date**: 2026-02-27 | **Spec**: [spec.md](spec.md)
**Input**: Feature specification from `/kitty-specs/001-weak-driven-prompt-optimization-experiment/spec.md`

## Summary

This feature implements the Large-Small-Large (LSL) prompt optimization framework using DSPy, HuggingFace datasets, Langfuse, and OpenRouter. It evaluates if prompts optimized on a weak model (Step-3.5-Flash) generalize better to a frontier model (Trinity-Large-Preview) than self-optimized prompts.

## Technical Context

**Language/Version**: Python 3.12
**Primary Dependencies**: DSPy, HuggingFace datasets, Langfuse, OpenRouter (OpenAI-compatible client), Pydantic
**Storage**: Local JSON/JSONL files for experiment runs, prompt candidates, evaluation results, and raw model outputs (datasets). Langfuse for tracing.
**Testing**: Black-box integration tests with mocked LLM calls (testing the full DSPy optimization workflow).
**Target Platform**: Localhost CLI/batch execution.
**Project Type**: Python script/CLI application.
**Performance Goals**: Maximize parallel execution for I/O bound OpenRouter calls.
**Constraints**: Requires OpenRouter API Key and Langfuse API Key. Strict schema validation via Pydantic.
**Scale/Scope**: Running benchmark evaluations (FRONTIERMATH, SuperGPQA, tau^2-Bench) across multiple prompt iterations.

## Constitution Check

*GATE: Must pass before Phase 0 research. Re-check after Phase 1 design.*

- **Python 3.12 with DSPy**: Passed.
- **Strict schema validation**: Passed (will use Pydantic).
- **On-cloud langfuse for observability**: Passed.
- **Black-box integration tests**: Passed (will mock LLMs for testing full workflow).
- **No unit tests**: Passed.
- **Batch execution & parallel OpenRouter calls**: Passed.
- **Localhost deployment**: Passed.

## Project Structure

### Documentation (this feature)

```text
kitty-specs/001-weak-driven-prompt-optimization-experiment/
├── plan.md              # This file
├── research.md          # Phase 0 output
├── data-model.md        # Phase 1 output
├── quickstart.md        # Phase 1 output
├── contracts/           # Phase 1 output
└── tasks.md             # Phase 2 output (generated by /spec-kitty.tasks)
```

### Source Code (repository root)

```text
src/
├── evaluation/          # HF datasets loading & metric evaluation functions
├── optimization/        # DSPy GEPA optimizer and LSL pipeline logic
├── models/              # Pydantic schemas for state, prompts, and results
└── utils/               # Langfuse tracing setup, OpenRouter clients
tests/
└── integration/         # Black-box tests with mocked LLMs
main.py                  # CLI entrypoint for the experiment
```

**Structure Decision**: Selected a single Python project structure tailored for a CLI-based evaluation harness.

## Complexity Tracking

No constitution violations detected.